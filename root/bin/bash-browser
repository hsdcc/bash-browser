#!/bin/bash

get_term_size() {
  # This uses an alternative escape sequence (\e[18t) to get terminal size,
  # which has proven to be more reliable.
  # Response format: \e[8;<height>;<width>t
  printf '\e[18t' >/dev/tty

  local response
  read -s -t 1 -d t response </dev/tty

  if [[ -n "$response" && "$response" == *";"* && "$response" == *"8;"* ]]; then
    response=${response#*;} # Remove leading junk up to first ;
    term_height=${response%;*}
    term_width=${response#*;} # Remove leading junk up to first ;
  fi

  # Fallback if detection fails
  term_height=${term_height:-24}
  term_width=${term_width:-80}
}

if [ $# -eq 0 ]; then
  echo "usage: bash-browser <url>"
  exit 1
fi

url="$1"
if [[ ! "$url" == *"://"* ]]; then
  url="http://$url"
fi
echo "loading..."

echo "fetching: $url"

# Use the curl script to download content
content="$("/home/meow/code/bash-browser/root/bin/curl" "$url")"
if [ $? -ne 0 ]; then
  echo "error: could not fetch content from $url"
  exit 1
fi

# Basic HTML tag stripping and decoding, optimized to run on the whole content at once.

# 1. Remove HTML comments using pure bash
# This is still iterative, but runs on the whole buffer, not line-by-line
while [[ "$content" == *"<!--"*"-->"* ]]; do
  start_comment="${content%%<!--*}"
  end_comment="${content#*-->}"
  content="${start_comment}${end_comment}"
done

# 1.5. Process <a> tags to make links visible: <a href="URL">TEXT</a> -> TEXT [URL]
# Since HTML tags can span multiple lines, we'll convert to single line temporarily, process, then restore
# Use a unique marker that won't appear in HTML content

LINE_MARKER="___BASHBROWSER_LINEBREAK___"

# Convert newlines to markers to allow multi-line pattern matching
temp_content="${content//$'\n'/$LINE_MARKER}"

# Process <A HREF= tags (upper case)
# Since newlines are converted to markers, we need to find <A that might have markers before attributes
# This pattern now looks for <A followed by any amount of content (including markers) to HREF=
original_temp="$temp_content"
while [[ "$temp_content" == *"<A"* ]]; do
  # Find the complete opening tag structure by looking for <A, then everything up to the first >, then content to </A>
  if [[ "$temp_content" != *"<A"*">"*"</A>"* ]]; then
    break  # No complete <A>...</A> tag found
  fi
  
  before_a="${temp_content%%<A*}"
  after_a="${temp_content#*<A}"
  
  # Get the part up to first > (the tag attributes) and part after > (the content)
  if [[ "$after_a" != *">"* ]]; then
    break  # No closing > found
  fi
  
  tag_attrs_part="${after_a%%>*}"
  content_after_opening="${after_a#*>}"
  
  # Check if HREF exists in the tag attributes
  if [[ "$tag_attrs_part" == *"HREF="* ]]; then
    # Extract the href URL
    after_href="${tag_attrs_part#*HREF=}"
    quote_char="${after_href:0:1}"
    
          if [[ "$quote_char" == '"' || "$quote_char" == "'" ]]; then
            url_part="${after_href:1}"
            href_url="${url_part%%"$quote_char"*}"
          else
            href_url="${after_href%%[[:space:]>\<]*}"
          fi
        
        # Now find the content within the tag and the part after closing </A>
        if [[ "$content_after_opening" != *"</A>"* ]]; then
          break  # No closing </A> found
        fi
        
        anchor_content="${content_after_opening%%</A>*}"
        after_closing="${content_after_opening#*</A>}"
        
        # Construct the replacement
        full_opening_tag="<A${tag_attrs_part}>"
        full_pattern="${full_opening_tag}${anchor_content}</A>"
        replacement="${anchor_content} [${href_url}]"    
    temp_content="${temp_content/"$full_pattern"/"$replacement"}"
  else
    # No HREF in this tag, break to avoid infinite loop
    break
  fi
  
  # Prevent infinite loop
  if [[ "$temp_content" == "$original_temp" ]]; then
    break
  fi
  original_temp="$temp_content"
done

# Process <a href= tags (lower case) similarly
original_temp="$temp_content"
while [[ "$temp_content" == *"<a"* ]]; do
  if [[ "$temp_content" != *"<a"*">"*"</a>"* ]]; then
    break
  fi
  
  before_a="${temp_content%%<a*}"
  after_a="${temp_content#*<a}"
  
  if [[ "$after_a" != *">"* ]]; then
    break
  fi
  
  tag_attrs_part="${after_a%%>*}"
  content_after_opening="${after_a#*>}"
  
  if [[ "$tag_attrs_part" == *"href="* ]]; then
    after_href="${tag_attrs_part#*href=}"
    quote_char="${after_href:0:1}"
    
    if [[ "$quote_char" == '"' || "$quote_char" == "'" ]]; then
      url_part="${after_href:1}"
      href_url="${url_part%%"$quote_char"*}"
    else
      href_url="${after_href%%[[:space:]>\<]*}"
    fi
    
    if [[ "$content_after_opening" != *"</a>"* ]]; then
      break
    fi
    
    anchor_content="${content_after_opening%%</a>*}"
    after_closing="${content_after_opening#*</a>}"
    
    full_opening_tag="<a${tag_attrs_part}>"
    full_pattern="${full_opening_tag}${anchor_content}</a>"
    replacement="${anchor_content} [${href_url}]"
    
    temp_content="${temp_content/"$full_pattern"/"$replacement"}"
  else
    break
  fi
  
  if [[ "$temp_content" == "$original_temp" ]]; then
    break
  fi
  original_temp="$temp_content"
done

# Convert back from markers to actual newlines
content="${temp_content//$LINE_MARKER/$'\n'}"

# 2. Replace certain closing tags with newlines before removing all other tags
# Handle h1-h6 tags
content="${content//<\/h1>/$'\n'$'\n'}"
content="${content//<\/h2>/$'\n'$'\n'}"
content="${content//<\/h3>/$'\n'$'\n'}"
content="${content//<\/h4>/$'\n'$'\n'}"
content="${content//<\/h5>/$'\n'$'\n'}"
content="${content//<\/h6>/$'\n'$'\n'}"
# Other block-level tags
content="${content//<\/p>/$'\n'$'\n'}"
content="${content//<br>/$'\n'}"
content="${content//<br\/>/$'\n'}"
content="${content//<br \/>/$'\n'}"
content="${content//<\/li>/$'\n'}"
content="${content//<\/div>/$'\n'}"
content="${content//<\/tr>/$'\n'}"
content="${content//<\/td>/	}" # tab character
content="${content//<\/th>/	}" # tab character

# 3. Remove all remaining HTML tags iteratively
# This is the main bottleneck, but necessary for correctness in pure bash
while [[ "$content" == *"<"*">"* ]]; do
  start_part="${content%%<*}"
  remaining="${content#*>}"
  content="${start_part}${remaining}"
done

# 4. Decode common HTML entities
content="${content//&amp;/&}"
content="${content//&lt;/<}"
content="${content//&gt;/>}"
content="${content//&quot;/\"}"
content="${content//&#39;/\'}"

# 5. Clean up extra blank lines
# The previous steps can create lots of consecutive newlines, this collapses them.
# Replace three or more newlines with two.
while [[ "$content" == *$'\n\n\n'* ]]; do
  content="${content//$'\n\n\n'/$'\n\n'}"
done

# --- scrolling implementation ---

follow_links() {
  local visible_links=()
  local link_positions=()
  local hint_chars="asdfjklgh"
  declare -A hint_lookup

  # Pattern to find http(s) URLs, excluding common trailing characters
  local url_pattern='https?://[^][<>"[:space:]]*'
  
  # Pattern to find [URL] style links created from HTML <a> tags
  local bracket_url_pattern='\[([^]]*)\]'

  for ((i = 0; i < view_height; i++)); do
    local line_index=$((offset + i))
    [ $line_index -ge $line_count ] && break

    local line_content="${lines[$line_index]}"
    local col_offset=0

    # First look for http(s) URLs
    while [[ "$line_content" =~ $url_pattern ]]; do
      local found_url="${BASH_REMATCH[0]}"
      local pre_match="${line_content%%"$found_url"*}"
      local col=$((col_offset + ${#pre_match} + 1))

      visible_links+=("$found_url")
      link_positions+=("$((i + 1));$col")

      local consumed_part="${pre_match}${found_url}"
      line_content="${line_content#"${consumed_part}"}"
      col_offset=$((col_offset + ${#consumed_part}))
    done
    
    # Reset line content to look for bracket URLs 
    line_content="${lines[$line_index]}"
    col_offset=0
    
    # Then look for bracket-style URLs like [WhatIs.html]
    while [[ "$line_content" =~ $bracket_url_pattern ]]; do
      # Extract the URL between brackets
      local full_match="${BASH_REMATCH[0]}"
      local found_url="${BASH_REMATCH[1]}"
      
      local pre_match="${line_content%%"$full_match"*}"
      local col=$((col_offset + ${#pre_match} + 1))

      visible_links+=("$found_url")
      link_positions+=("$((i + 1));$col")

      local consumed_part="${pre_match}${full_match}"
      line_content="${line_content#"${consumed_part}"}"
      col_offset=$((col_offset + ${#consumed_part}))
    done
  done

  if [ ${#visible_links[@]} -eq 0 ]; then
    printf "\e[${height};0H\e[41m"
    printf "%-${term_width}s" "No links found on screen."
    printf "\e[0m"
    sleep 1
    return
  fi

  for i in "${!visible_links[@]}"; do
    local pos="${link_positions[$i]}"
    local screen_line="${pos%%;*}"
    local screen_col="${pos#*;}"
    local hint_char="${hint_chars:$i:1}"

    [ -z "$hint_char" ] && break

    hint_lookup["$hint_char"]="$i"

    printf "\e[%s;%sH\e[42;30m[%s]\e[0m" "$screen_line" "$screen_col" "$hint_char"
  done

  printf "\e[${height};0H\e[44m"
  printf "%-${term_width}s" "Type hint to follow link, or any other key to cancel."
  printf "\e[0m"

  read -s -n 1 key

  local link_index="${hint_lookup[$key]}"
  if [[ -n "$link_index" ]]; then
    local target_url="${visible_links[$link_index]}"

    printf "\e[?25h\033c"

    # --- NEW URL RESOLUTION LOGIC ---

    # Ensure base URL is absolute
    local base_url="$url"
    if [[ ! "$base_url" == *"://"* ]]; then
      # This is a safeguard. The script should ensure url is always absolute.
      # If it's not, we can't reliably resolve relative paths.
      # We'll attempt to fix it, but this indicates a deeper problem.
      base_url="http://$base_url"
    fi

    # If target is already a full URL, we're done.
    if [[ "$target_url" == "http"* || "$target_url" == "https"* ]]; then
      exec "$0" "$target_url"
      return
    fi

    local protocol="${base_url%%://*}"
    local rest="${base_url#*://}"
    local host="${rest%%/*}"
    local path_part="${rest#$host}"

    # If target is protocol-relative (e.g., //example.com), prepend protocol
    if [[ "$target_url" == //* ]]; then
      exec "$0" "${protocol}:${target_url}"
      return
    fi

    # If target is root-relative (e.g., /foo/bar), combine with host
    if [[ "$target_url" == /* ]]; then
      exec "$0" "${protocol}://${host}${target_url}"
      return
    fi

    # It's a document-relative link (e.g., "foo.html" or "../foo.html")
    # Get the directory of the current path
    local current_dir
    if [[ "$path_part" == */ ]]; then
      current_dir="$path_part"
    else
      current_dir="${path_part%/*}"
    fi
    
    # If current_dir is empty, it means we are at the root path.
    if [ -z "$current_dir" ]; then
      current_dir="/"
    fi
    
    # Ensure current_dir ends with a slash for proper joining
    if [[ ! "$current_dir" == */ ]]; then
      current_dir="$current_dir/"
    fi

    local new_path="${current_dir}${target_url}"

    # Normalize the new path (handle .. and .)
    local OIFS="$IFS"
    IFS='/'
    read -ra path_segments <<< "$new_path"
    IFS="$OIFS"
    
    local final_segments=()
    for segment in "${path_segments[@]}"; do
      if [[ "$segment" == ".." ]]; then
        if [ ${#final_segments[@]} -gt 0 ]; then
          # Don't pop the initial empty segment for the root
          if ! { [ ${#final_segments[@]} -eq 1 ] && [ -z "${final_segments[0]}" ]; }; then
            final_segments=("${final_segments[@]:0:${#final_segments[@]}-1}")
          fi
        fi
      elif [[ "$segment" != "." && -n "$segment" ]]; then
        final_segments+=("$segment")
      # Handle the case where the path is just "/" or similar
      elif [[ -z "$segment" && ${#final_segments[@]} -eq 0 ]]; then
        final_segments+=("")
      fi
    done

    # Reconstruct the final path from segments
    OIFS="$IFS"
    IFS='/'
    final_path="${final_segments[*]}"
    IFS="$OIFS"

    # Ensure it's a valid path
    if [[ -z "$final_path" ]]; then
        final_path="/"
    # If it was absolute, it should start with /. If not, add it.
    elif [[ "${final_segments[0]}" == "" && "${final_path:0:1}" != "/" ]]; then
        final_path="/$final_path"
    fi

    exec "$0" "${protocol}://${host}${final_path}"
  fi
  # If key is not a valid hint, we do nothing and the main loop will redraw the screen.
}

# store content in an array
mapfile -t lines <<<"$content"

# get terminal size
get_term_size
height=${term_height}

# leave room for status bar
view_height=$((height - 1))

offset=0
line_count=${#lines[@]}

# hide cursor
printf "\e[?25l"

while true; do
  # clear screen
  printf "\033c"

  # display content
  for ((i = 0; i < view_height; i++)); do
    line_index=$((offset + i))
    if [ $line_index -lt $line_count ]; then
      echo "${lines[$line_index]}"
    fi
  done

  # status bar
  printf "\e[${height};0H" # move to last line
  printf "\e[44m"          # blue background
  status_text=$(printf "%s" "$url")
  printf "%-${term_width}s" "$status_text" # pad to full width
  printf "\e[0m"                           # reset color

  # get user input
  read -s -n 1 key

  case "$key" in
  'q')
    break
    ;;
  'j')
    # scroll down
    if [ $((offset + view_height)) -lt $line_count ]; then
      offset=$((offset + 1))
    fi
    ;;
  'k')
    # scroll up
    if [ $offset -gt 0 ]; then
      offset=$((offset - 1))
    fi
    ;;
  's')
    follow_links
    ;;
  esac
done

# show cursor and clear screen on exit
printf "\e[?25h"
printf "\033c"
